import streamlit as st
import pandas as pd
from core.db_connector import get_redshift_data, get_postgres_data, get_mysql_data # Relative import
from tools_tableau.llm_services import LLMClientInterface # Relative import
import numpy as np # For potential histogram binning

# --- Database Helper Functions ---

DB_FUNCTIONS = {
    "PostgreSQL": get_postgres_data,
    "Redshift": get_redshift_data,
    "MySQL": get_mysql_data,
}

# Database-specific schema queries (adjust schema names if needed)
SCHEMA_QUERIES = {
    "PostgreSQL": "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = %s ORDER BY ordinal_position;",
    "Redshift": "SELECT \"column\" AS column_name, type AS data_type FROM pg_table_def WHERE tablename = %s ORDER BY \"column\";", # Redshift uses pg_table_def
    "MySQL": "SELECT COLUMN_NAME as column_name, DATA_TYPE as data_type FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = %s ORDER BY ORDINAL_POSITION;", # Assumes current database
}

def get_db_function(db_type):
    """Get the appropriate data fetching function based on db_type."""
    return DB_FUNCTIONS.get(db_type)

def run_query(db_type, sql_query, params=None):
    """Runs a query using the correct database function."""
    db_func = get_db_function(db_type)
    if not db_func:
        raise ValueError(f"Unsupported database type: {db_type}")
    try:
        return db_func(sql_query, params)
    except Exception as e:
        st.error(f"è³‡æ–™åº«æŸ¥è©¢éŒ¯èª¤ ({db_type}): {e}")
        # Optionally re-raise or return None/empty DataFrame
        # raise # Re-raise if you want the main function to handle it further
        return pd.DataFrame() # Return empty DataFrame to prevent further errors

def get_table_schema(db_type, table_name):
    """Fetches the table schema (column names and data types)."""
    query = SCHEMA_QUERIES.get(db_type)
    if not query:
        st.error(f"ä¸æ”¯æ´å–å¾— {db_type} çš„ schema è³‡è¨Šã€‚")
        return pd.DataFrame()
    return run_query(db_type, query, (table_name,))

def get_row_count(db_type, table_name):
    """Gets the total row count of a table."""
    # Basic check for table name validity (prevent SQL injection)
    if not table_name or not table_name.isalnum() and '_' not in table_name:
         st.error(f"ç„¡æ•ˆçš„è¡¨åç¨±: {table_name}")
         return 0
    # Use f-string carefully ONLY after validation, or prefer parameterized queries if connector supports it for table names (most don't)
    query = f"SELECT COUNT(*) as total_rows FROM {table_name};"
    df = run_query(db_type, query)
    if not df.empty:
        return df['total_rows'][0]
    return 0

# --- Data Type Mapping ---
def map_data_type(db_type_str):
    """Maps database-specific data types to general categories."""
    db_type_str = db_type_str.lower()
    if any(t in db_type_str for t in ['timestamp', 'date', 'time']):
        return 'datetime'
    if any(t in db_type_str for t in ['int', 'numeric', 'decimal', 'float', 'double', 'real', 'long']):
        return 'numeric'
    if any(t in db_type_str for t in ['char', 'text', 'string', 'varchar']):
        return 'string'
    if 'bool' in db_type_str:
        return 'boolean'
    return 'other'

# --- EDA Helper Functions ---

def analyze_datetime_column(db_type, table_name, column_name):
    """Performs EDA for datetime columns."""
    # Basic check for column name validity
    if not column_name or not column_name.isalnum() and '_' not in column_name:
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return

    query_stats = f"SELECT MIN({column_name}) as min_date, MAX({column_name}) as max_date FROM {table_name};"
    df_stats = run_query(db_type, query_stats)

    if not df_stats.empty:
        st.metric("æœ€æ—©æ—¥æœŸ", str(df_stats['min_date'][0]))
        st.metric("æœ€æ™šæ—¥æœŸ", str(df_stats['max_date'][0]))

    # Monthly distribution (consider sampling for large tables)
    # Database-specific date truncation is more efficient than pulling all data
    date_trunc_options = {
        "PostgreSQL": f"SELECT date_trunc('month', {column_name})::date as month_start, COUNT(*) as count FROM {table_name} WHERE {column_name} IS NOT NULL GROUP BY 1 ORDER BY 1;",
        "Redshift": f"SELECT date_trunc('month', {column_name})::date as month_start, COUNT(*) as count FROM {table_name} WHERE {column_name} IS NOT NULL GROUP BY 1 ORDER BY 1;",
        "MySQL": f"SELECT DATE_FORMAT({column_name}, '%Y-%m-01') as month_start, COUNT(*) as count FROM {table_name} WHERE {column_name} IS NOT NULL GROUP BY 1 ORDER BY 1;"
    }
    query_dist = date_trunc_options.get(db_type)
    if query_dist:
        df_dist = run_query(db_type, query_dist)
        if not df_dist.empty:
            st.write("#### æ¯æœˆè³‡æ–™ç­†æ•¸åˆ†ä½ˆ")
            df_dist.set_index('month_start', inplace=True)
            st.bar_chart(df_dist['count'])
        else:
            st.write("ç„¡æ³•è¨ˆç®—æ¯æœˆåˆ†ä½ˆã€‚")
    else:
        st.write(f"ä¸æ”¯æ´ {db_type} çš„æ¯æœˆåˆ†ä½ˆæŸ¥è©¢ã€‚")


def analyze_numeric_column(db_type, table_name, column_name):
    """Performs EDA for numeric columns."""
    if not column_name or not column_name.isalnum() and '_' not in column_name:
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return

    # Basic stats
    query_stats = f"SELECT MIN({column_name}) as min_val, MAX({column_name}) as max_val, AVG({column_name}) as avg_val FROM {table_name} WHERE {column_name} IS NOT NULL;"
    df_stats = run_query(db_type, query_stats)

    if not df_stats.empty:
        col1, col2, col3 = st.columns(3)
        col1.metric("æœ€å°å€¼", f"{df_stats['min_val'][0]:,.2f}" if pd.notna(df_stats['min_val'][0]) else "N/A")
        col2.metric("æœ€å¤§å€¼", f"{df_stats['max_val'][0]:,.2f}" if pd.notna(df_stats['max_val'][0]) else "N/A")
        col3.metric("å¹³å‡å€¼", f"{df_stats['avg_val'][0]:,.2f}" if pd.notna(df_stats['avg_val'][0]) else "N/A")

    # Distribution (Histogram - Sample data for large tables)
    st.write("#### æ•¸å€¼åˆ†ä½ˆåœ– (æŠ½æ¨£å‰ 10000 ç­†éç©ºå€¼)")
    # Consider adding TABLESAMPLE if supported and needed for performance
    query_sample = f"SELECT {column_name} FROM {table_name} WHERE {column_name} IS NOT NULL LIMIT 10000;"
    df_sample = run_query(db_type, query_sample)

    if not df_sample.empty:
        try:
            # Attempt to convert to numeric, coercing errors
            numeric_data = pd.to_numeric(df_sample[column_name], errors='coerce').dropna()
            if not numeric_data.empty:
                # Simple histogram using numpy and st.bar_chart
                counts, bins = np.histogram(numeric_data, bins=10) # Adjust bins as needed
                bin_labels = [f"{bins[i]:.1f}-{bins[i+1]:.1f}" for i in range(len(bins)-1)]
                hist_df = pd.DataFrame({'count': counts}, index=bin_labels)
                st.bar_chart(hist_df)
            else:
                 st.write("æ¬„ä½è³‡æ–™ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼é€²è¡Œåˆ†ä½ˆåˆ†æã€‚")
        except Exception as e:
            st.warning(f"ç¹ªè£½æ•¸å€¼åˆ†ä½ˆåœ–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        st.write("ç„¡æ³•å–å¾—æ•¸å€¼æ¨£æœ¬è³‡æ–™ã€‚")


def analyze_string_column(db_type, table_name, column_name):
    """Performs EDA for string columns."""
    if not column_name or not column_name.isalnum() and '_' not in column_name:
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return

    # Get distinct count
    query_distinct_count = f"SELECT COUNT(DISTINCT {column_name}) as distinct_count FROM {table_name};"
    df_distinct_count = run_query(db_type, query_distinct_count)
    distinct_count = 0
    if not df_distinct_count.empty:
        distinct_count = df_distinct_count['distinct_count'][0]

    st.metric("ä¸é‡è¤‡å€¼æ•¸é‡", distinct_count)

    if distinct_count == 0:
        st.write("æ¬„ä½ç„¡è³‡æ–™æˆ–çš†ç‚º NULLã€‚")
        return

    if distinct_count <= 30:
        st.write("#### å€¼åˆ†ä½ˆ (Top 30)")
        query_counts = f"SELECT {column_name}, COUNT(*) as count FROM {table_name} WHERE {column_name} IS NOT NULL GROUP BY {column_name} ORDER BY count DESC LIMIT 30;"
        df_counts = run_query(db_type, query_counts)
        if not df_counts.empty:
            total_rows_non_null = df_counts['count'].sum() # Use sum of counts as total (approx if LIMIT < total)
            df_counts['ä½”æ¯” (%)'] = (df_counts['count'] / total_rows_non_null * 100).round(2)
            df_counts.set_index(column_name, inplace=True)
            st.dataframe(df_counts)
            st.bar_chart(df_counts['count'])
        else:
             st.write("ç„¡æ³•å–å¾—å€¼åˆ†ä½ˆè³‡æ–™ã€‚")
    else:
        st.write(f"#### éš¨æ©Ÿæ¨£æœ¬ (30 ç­†)")
        st.info(f"ç”±æ–¼ä¸é‡è¤‡å€¼æ•¸é‡ ({distinct_count}) éå¤šï¼Œåƒ…é¡¯ç¤ºéš¨æ©Ÿæ¨£æœ¬ã€‚")
        # Sampling method depends on DB. LIMIT is simple but not random.
        # ORDER BY RANDOM() is expensive. Use simple LIMIT for now.
        query_sample = f"SELECT DISTINCT {column_name} FROM {table_name} WHERE {column_name} IS NOT NULL LIMIT 30;"
        df_sample = run_query(db_type, query_sample)
        if not df_sample.empty:
            st.dataframe(df_sample)
        else:
             st.write("ç„¡æ³•å–å¾—æ¨£æœ¬è³‡æ–™ã€‚")

def analyze_boolean_column(db_type, table_name, column_name):
    """Performs EDA for boolean columns."""
    if not column_name or not column_name.isalnum() and '_' not in column_name:
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return

    st.write("#### å€¼åˆ†ä½ˆ")
    query_counts = f"SELECT {column_name}, COUNT(*) as count FROM {table_name} WHERE {column_name} IS NOT NULL GROUP BY {column_name};"
    df_counts = run_query(db_type, query_counts)

    if not df_counts.empty:
        total_rows_non_null = df_counts['count'].sum()
        df_counts['ä½”æ¯” (%)'] = (df_counts['count'] / total_rows_non_null * 100).round(2)
        # Ensure boolean values are displayed nicely
        df_counts[column_name] = df_counts[column_name].astype(str)
        df_counts.set_index(column_name, inplace=True)
        st.dataframe(df_counts)
        st.bar_chart(df_counts['count'])
    else:
        st.write("ç„¡æ³•å–å¾—å€¼åˆ†ä½ˆè³‡æ–™ã€‚")


# --- LLM Helper Functions ---
def generate_translation_prompt(column_name):
    """Generates prompt for translating column name."""
    return f"è«‹å°‡ä»¥ä¸‹æŠ€è¡“æ€§çš„è³‡æ–™åº«æ¬„ä½åç¨±ç¿»è­¯æˆåœ¨å°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡æ¥­å‹™æ„ç¾©ï¼Œè«‹ç›¡é‡ç°¡çŸ­ï¼Œåªå›å‚³ç¿»è­¯çµæœï¼š\næ¬„ä½åç¨±ï¼š{column_name}\nå¯èƒ½çš„æ„ç¾©ï¼š"

def generate_relations_prompt(schema_df):
    """Generates prompt for suggesting related columns."""
    schema_str = "\n".join([f"- {row['column_name']} ({row['data_type']})" for index, row in schema_df.iterrows()])
    return f"""
    ä»¥ä¸‹æ˜¯è³‡æ–™è¡¨ä¸­çš„æ¬„ä½åŠå…¶è³‡æ–™é¡å‹ï¼š
    {schema_str}

    è«‹æ ¹æ“šé€™äº›æ¬„ä½åç¨±å’Œé¡å‹ï¼Œæ¨æ¸¬ä¸¦å»ºè­°å“ªäº›æ¬„ä½ä¹‹é–“å¯èƒ½å­˜åœ¨é—œè¯æ€§ï¼Œæˆ–è€…å“ªäº›æ¬„ä½çµ„åˆåœ¨ä¸€èµ·åˆ†æå¯èƒ½æœƒæœ‰æ„ç¾©ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡çŸ­è§£é‡‹åŸå› ã€‚

    å¯èƒ½çš„é—œè¯æˆ–çµ„åˆå»ºè­°ï¼š
    """

# --- Main Streamlit Function ---
def show(llm_client: LLMClientInterface, model_name: str):
    """ä¸»å‡½æ•¸ï¼Œé¡¯ç¤ºè³‡æ–™è¡¨æ¢ç´¢å·¥å…·çš„ä»‹é¢èˆ‡é‚è¼¯ã€‚"""
    st.markdown("##### è³‡æ–™è¡¨æ¢ç´¢å·¥å…·")
    st.write("é¸æ“‡è³‡æ–™åº«å’Œè¡¨åï¼Œæ¢ç´¢è¡¨çš„çµæ§‹ã€åŸºæœ¬è³‡è¨Šå’Œæ¬„ä½å…§å®¹åˆ†ä½ˆã€‚")

    # --- Inputs ---
    db_type = st.selectbox("é¸æ“‡è³‡æ–™åº«é¡å‹", options=list(DB_FUNCTIONS.keys()))
    table_name = st.text_input("è¼¸å…¥è¦æ¢ç´¢çš„è¡¨å", help="è«‹è¼¸å…¥è³‡æ–™åº«ä¸­çš„è¡¨åï¼Œå€åˆ†å¤§å°å¯«ã€‚")

    if st.button("ğŸš€ é–‹å§‹æ¢ç´¢", type="primary"):
        if not db_type:
            st.warning("è«‹é¸æ“‡è³‡æ–™åº«é¡å‹ã€‚")
            st.stop()
        if not table_name:
            st.warning("è«‹è¼¸å…¥è¡¨åã€‚")
            st.stop()

        with st.spinner(f"æ­£åœ¨é€£æ¥ {db_type} ä¸¦è®€å–è³‡æ–™è¡¨è³‡è¨Š..."):
            # --- 1. åŸºç¤è³‡è¨Š & Schema ---
            st.markdown("---")
            st.markdown("### 1. è³‡æ–™è¡¨åŸºç¤è³‡è¨Š & æ¬„ä½çµæ§‹")

            row_count = get_row_count(db_type, table_name)
            st.metric("ç¸½è³‡æ–™ç­†æ•¸", f"{row_count:,}")

            schema_df = get_table_schema(db_type, table_name)

            if schema_df.empty:
                st.error(f"ç„¡æ³•è®€å–è¡¨ '{table_name}' çš„çµæ§‹ï¼Œè«‹ç¢ºèªè¡¨åå’Œæ¬Šé™ã€‚")
                st.stop()

            st.write(f"ç¸½æ¬„ä½æ•¸: {len(schema_df)}")

            # Get translations using LLM (add error handling)
            translations = []
            meanings_placeholder = st.empty() # Placeholder for status
            with st.spinner("æ­£åœ¨é€é AI æ¨æ¸¬æ¬„ä½æ„ç¾©..."):
                for index, row in schema_df.iterrows():
                    try:
                        prompt = generate_translation_prompt(row['column_name'])
                        translation = llm_client.generate_text(prompt, model_name, temperature=0.1)
                        translations.append(translation.strip())
                    except Exception as e:
                        print(f"ç¿»è­¯æ¬„ä½ {row['column_name']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        translations.append("ç¿»è­¯å¤±æ•—") # Append placeholder on error
                        # Optionally add a warning to the UI for the user
                meanings_placeholder.success("æ¬„ä½æ„ç¾©æ¨æ¸¬å®Œæˆï¼")

            schema_df['æ¨æ¸¬æ„ç¾© (AI)'] = translations
            schema_df['é€šç”¨é¡å‹'] = schema_df['data_type'].apply(map_data_type)

            st.dataframe(schema_df)


            # --- 5. æ¬„ä½é—œè¯æ€§å»ºè­° (LLM) ---
            st.markdown("---")
            st.markdown("### 2. æ¬„ä½é—œè¯æ€§å»ºè­° (AI)")
            with st.spinner("æ­£åœ¨é€é AI åˆ†ææ¬„ä½é–“å¯èƒ½çš„é—œè¯..."):
                 try:
                     relations_prompt = generate_relations_prompt(schema_df)
                     relations_suggestion = llm_client.generate_text(relations_prompt, model_name, temperature=0.5)
                     st.markdown(relations_suggestion)
                 except Exception as e:
                     st.error(f"åˆ†ææ¬„ä½é—œè¯æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


            # --- 4. æ¬„ä½æ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA) ---
            st.markdown("---")
            st.markdown("### 3. æ¬„ä½æ¢ç´¢æ€§åˆ†æ (EDA)")
            st.info("é»æ“Šå±•é–‹å„æ¬„ä½æŸ¥çœ‹è©³ç´°åˆ†æã€‚")

            if not schema_df.empty:
                for index, row in schema_df.iterrows():
                    col_name = row['column_name']
                    col_db_type = row['data_type']
                    col_general_type = row['é€šç”¨é¡å‹']

                    with st.expander(f"æ¬„ä½: **{col_name}** (é¡å‹: {col_db_type} / {col_general_type})"):
                        with st.spinner(f"æ­£åœ¨åˆ†ææ¬„ä½ {col_name}..."):
                            try:
                                if col_general_type == 'datetime':
                                    analyze_datetime_column(db_type, table_name, col_name)
                                elif col_general_type == 'numeric':
                                    analyze_numeric_column(db_type, table_name, col_name)
                                elif col_general_type == 'string':
                                    analyze_string_column(db_type, table_name, col_name)
                                elif col_general_type == 'boolean':
                                    analyze_boolean_column(db_type, table_name, col_name)
                                else:
                                    st.write("æ­¤è³‡æ–™é¡å‹å°šä¸æ”¯æ´è‡ªå‹• EDA åˆ†æã€‚")
                            except Exception as e:
                                st.error(f"åˆ†ææ¬„ä½ '{col_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            else:
                st.warning("ç„¡æ³•åŸ·è¡Œ EDAï¼Œå› ç‚ºæœªèƒ½è®€å–è³‡æ–™è¡¨çµæ§‹ã€‚")
