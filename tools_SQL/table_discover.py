import streamlit as st
import pandas as pd
# Assuming core and tools_tableau are siblings under the main project root
from core.db_connector import get_redshift_data, get_postgres_data, get_mysql_data
from core.llm_services import LLMClientInterface # Adjusted path
import numpy as np

# --- Database Helper Functions ---

DB_FUNCTIONS = {
    "Redshift": get_redshift_data,
    "PostgreSQL": get_postgres_data,
    "MySQL": get_mysql_data,
}

# Database-specific queries for TABLES
TABLE_SCHEMA_QUERIES = {
    "Redshift": "SELECT \"column\" AS column_name, type AS data_type FROM pg_table_def WHERE schemaname = %s AND tablename = %s ORDER BY \"column\";",
    "PostgreSQL": "SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = %s AND table_name = %s AND table_schema NOT IN ('information_schema', 'pg_catalog') ORDER BY ordinal_position;",
    "MySQL": "SELECT COLUMN_NAME as column_name, DATA_TYPE as data_type FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s ORDER BY ORDINAL_POSITION;",
}

# Database-specific queries for VIEWS
# ä¿®æ”¹ Redshift çš„ VIEW æŸ¥è©¢ç‚ºç¢ºèªå¯ç”¨çš„ svv_columns ç‰ˆæœ¬
VIEW_SCHEMA_QUERIES = {
     "Redshift": """
        SELECT column_name, data_type
        FROM svv_columns
        WHERE table_schema = %s
          AND table_name = %s
        ORDER BY ordinal_position;
     """,
    "PostgreSQL": "SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = %s AND table_name = %s AND table_schema NOT IN ('information_schema', 'pg_catalog') ORDER BY ordinal_position;",
    "MySQL": "SELECT COLUMN_NAME as column_name, DATA_TYPE as data_type FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s ORDER BY ORDINAL_POSITION;",
}


def get_db_function(db_type):
    """Get the appropriate data fetching function based on db_type."""
    return DB_FUNCTIONS.get(db_type)

def run_query(db_type, sql_query, params=None):
    """Runs a query using the correct database function."""
    db_func = get_db_function(db_type)
    if not db_func:
        raise ValueError(f"Unsupported database type: {db_type}")
    try:
        return db_func(sql_query, params)
    except Exception as e:
        st.error(f"è³‡æ–™åº«æŸ¥è©¢éŒ¯èª¤ ({db_type}): {e}")
        return pd.DataFrame()

# Modified to accept object_type
def get_object_schema(db_type, schema_name, object_name, object_type='TABLE'):
    """Fetches the schema (column names and data types) for a table or view."""
    if object_type == 'TABLE':
        query_dict = TABLE_SCHEMA_QUERIES
    elif object_type == 'VIEW':
        query_dict = VIEW_SCHEMA_QUERIES
    else:
        st.error(f"ä¸æ”¯æ´çš„ç‰©ä»¶é¡å‹: {object_type}")
        return pd.DataFrame()

    query = query_dict.get(db_type)
    if not query:
        st.error(f"ä¸æ”¯æ´å–å¾— {db_type} çš„ {object_type} schema è³‡è¨Šã€‚")
        return pd.DataFrame()

    # å‚³é schema_name å’Œ object_name ä½œç‚ºåƒæ•¸
    return run_query(db_type, query, (schema_name, object_name))

# Modified to accept object_name and type for clarity, though query is same
def get_object_row_count(db_type, schema_name, object_name, object_type='TABLE'):
    """Gets the total row count of a table or view."""
    # Combine schema and object name for the query, ensuring proper quoting if needed
    # Basic validation: Check if schema and object names are reasonably safe
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
    if not schema_name or not all(c in allowed_chars for c in schema_name):
        st.error(f"ç„¡æ•ˆçš„ Schema åç¨±: {schema_name}")
        return None
    if not object_name or not all(c in allowed_chars for c in object_name):
         st.error(f"ç„¡æ•ˆçš„ç‰©ä»¶åç¨±: {object_name}")
         return None

    # Construct qualified name (basic quoting for safety, might need adjustment based on DB rules)
    qualified_name = f"\"{schema_name}\".\"{object_name}\"" # Add quotes for Redshift/Postgres

    if object_type == 'VIEW':
        st.warning(f"æ­£åœ¨è¨ˆç®— VIEW '{qualified_name}' çš„ç¸½ç­†æ•¸ï¼Œé€™å¯èƒ½æœƒèŠ±è²»è¼ƒé•·æ™‚é–“...", icon="â³")

    query = f"SELECT COUNT(*) as total_rows FROM {qualified_name};"
    df = run_query(db_type, query)
    if not df.empty and pd.notna(df['total_rows'][0]):
        return df['total_rows'][0]
    return None # Return None if count fails or is NaN

# --- Data Type Mapping ---
# (map_data_type function remains the same)
def map_data_type(db_type_str):
    """Maps database-specific data types to general categories."""
    db_type_str = db_type_str.lower()
    if any(t in db_type_str for t in ['timestamp', 'date', 'time']):
        return 'datetime'
    if any(t in db_type_str for t in ['int', 'numeric', 'decimal', 'float', 'double', 'real', 'long', 'bigint', 'smallint', 'integer']):
        return 'numeric'
    if any(t in db_type_str for t in ['char', 'text', 'string', 'varchar', 'nvarchar']):
        return 'string'
    if 'bool' in db_type_str:
        return 'boolean'
    return 'other'

# --- EDA Helper Functions ---
# (analyze_* functions remain largely the same, accepting object_name)
# Modified signatures to accept object_name and type
def analyze_datetime_column(db_type, schema_name, object_name, column_name, object_type):
    """Performs EDA for datetime columns."""
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
    if not column_name or not all(c in allowed_chars for c in column_name):
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return
    if not schema_name or not all(c in allowed_chars for c in schema_name):
        st.error(f"ç„¡æ•ˆçš„ Schema åç¨±: {schema_name}")
        return
    if not object_name or not all(c in allowed_chars for c in object_name):
         st.error(f"ç„¡æ•ˆçš„ç‰©ä»¶åç¨±: {object_name}")
         return

    qualified_name = f"\"{schema_name}\".\"{object_name}\""
    # Use qualified name in queries
    query_stats = f"SELECT MIN(\"{column_name}\") as min_date, MAX(\"{column_name}\") as max_date FROM {qualified_name};"
    df_stats = run_query(db_type, query_stats)

    if not df_stats.empty:
        min_d = df_stats['min_date'][0]
        max_d = df_stats['max_date'][0]
        st.metric("æœ€æ—©æ—¥æœŸ", str(min_d) if pd.notna(min_d) else "N/A")
        st.metric("æœ€æ™šæ—¥æœŸ", str(max_d) if pd.notna(max_d) else "N/A")

    # Monthly distribution using qualified name
    date_trunc_options = {
        "PostgreSQL": f"SELECT date_trunc('month', \"{column_name}\")::date as month_start, COUNT(*) as count FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL GROUP BY 1 ORDER BY 1;",
        "Redshift": f"SELECT date_trunc('month', \"{column_name}\")::date as month_start, COUNT(*) as count FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL GROUP BY 1 ORDER BY 1;",
        "MySQL": f"SELECT DATE_FORMAT(`{column_name}`, '%Y-%m-01') as month_start, COUNT(*) as count FROM `{schema_name}`.`{object_name}` WHERE `{column_name}` IS NOT NULL GROUP BY 1 ORDER BY 1;" # MySQL uses backticks
    }
    query_dist = date_trunc_options.get(db_type)
    if query_dist:
        df_dist = run_query(db_type, query_dist)
        if not df_dist.empty:
            st.write("#### æ¯æœˆè³‡æ–™ç­†æ•¸åˆ†ä½ˆ")
            # Ensure month_start is suitable for indexing (string or datetime)
            try:
                df_dist['month_start'] = pd.to_datetime(df_dist['month_start']).dt.strftime('%Y-%m')
            except Exception:
                df_dist['month_start'] = df_dist['month_start'].astype(str) # Fallback to string
            df_dist.set_index('month_start', inplace=True)
            st.bar_chart(df_dist['count'])
        else:
            st.write("ç„¡æ³•è¨ˆç®—æ¯æœˆåˆ†ä½ˆã€‚")
    else:
        st.write(f"ä¸æ”¯æ´ {db_type} çš„æ¯æœˆåˆ†ä½ˆæŸ¥è©¢ã€‚")

def analyze_numeric_column(db_type, schema_name, object_name, column_name, object_type):
    """Performs EDA for numeric columns."""
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
    if not column_name or not all(c in allowed_chars for c in column_name):
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return
    if not schema_name or not all(c in allowed_chars for c in schema_name):
        st.error(f"ç„¡æ•ˆçš„ Schema åç¨±: {schema_name}")
        return
    if not object_name or not all(c in allowed_chars for c in object_name):
         st.error(f"ç„¡æ•ˆçš„ç‰©ä»¶åç¨±: {object_name}")
         return

    qualified_name = f"\"{schema_name}\".\"{object_name}\""
    query_stats = f"SELECT MIN(\"{column_name}\") as min_val, MAX(\"{column_name}\") as max_val, AVG(\"{column_name}\"::float) as avg_val FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL;" # Cast avg to float for wider compatibility
    df_stats = run_query(db_type, query_stats)

    if not df_stats.empty:
        col1, col2, col3 = st.columns(3)
        min_v = df_stats['min_val'][0]
        max_v = df_stats['max_val'][0]
        avg_v = df_stats['avg_val'][0]
        col1.metric("æœ€å°å€¼", f"{min_v:,.2f}" if pd.notna(min_v) else "N/A")
        col2.metric("æœ€å¤§å€¼", f"{max_v:,.2f}" if pd.notna(max_v) else "N/A")
        col3.metric("å¹³å‡å€¼", f"{avg_v:,.2f}" if pd.notna(avg_v) else "N/A")

    st.write("#### æ•¸å€¼åˆ†ä½ˆåœ– (æŠ½æ¨£å‰ 10000 ç­†éç©ºå€¼)")
    query_sample = f"SELECT \"{column_name}\" FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL LIMIT 10000;"
    df_sample = run_query(db_type, query_sample)

    if not df_sample.empty:
        try:
            numeric_data = pd.to_numeric(df_sample[column_name], errors='coerce').dropna()
            if not numeric_data.empty:
                counts, bins = np.histogram(numeric_data, bins=10)
                bin_labels = [f"{bins[i]:.1f}-{bins[i+1]:.1f}" for i in range(len(bins)-1)]
                hist_df = pd.DataFrame({'count': counts}, index=bin_labels)
                st.bar_chart(hist_df)
            else:
                 st.write("æ¬„ä½è³‡æ–™ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼é€²è¡Œåˆ†ä½ˆåˆ†æã€‚")
        except Exception as e:
            st.warning(f"ç¹ªè£½æ•¸å€¼åˆ†ä½ˆåœ–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        st.write("ç„¡æ³•å–å¾—æ•¸å€¼æ¨£æœ¬è³‡æ–™ã€‚")

def analyze_string_column(db_type, schema_name, object_name, column_name, object_type):
    """Performs EDA for string columns."""
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
    if not column_name or not all(c in allowed_chars for c in column_name):
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return
    if not schema_name or not all(c in allowed_chars for c in schema_name):
        st.error(f"ç„¡æ•ˆçš„ Schema åç¨±: {schema_name}")
        return
    if not object_name or not all(c in allowed_chars for c in object_name):
         st.error(f"ç„¡æ•ˆçš„ç‰©ä»¶åç¨±: {object_name}")
         return

    qualified_name = f"\"{schema_name}\".\"{object_name}\""
    query_distinct_count = f"SELECT COUNT(DISTINCT \"{column_name}\") as distinct_count FROM {qualified_name};"
    df_distinct_count = run_query(db_type, query_distinct_count)
    distinct_count = 0
    if not df_distinct_count.empty and pd.notna(df_distinct_count['distinct_count'][0]):
        distinct_count = df_distinct_count['distinct_count'][0]

    st.metric("ä¸é‡è¤‡å€¼æ•¸é‡", distinct_count)

    if distinct_count == 0:
        st.write("æ¬„ä½ç„¡è³‡æ–™æˆ–çš†ç‚º NULLã€‚")
        return

    if 0 < distinct_count <= 30:
        st.write("#### å€¼åˆ†ä½ˆ (Top 30)")
        query_counts = f"SELECT \"{column_name}\", COUNT(*) as count FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL GROUP BY \"{column_name}\" ORDER BY count DESC LIMIT 30;"
        df_counts = run_query(db_type, query_counts)
        if not df_counts.empty:
            total_rows_non_null = df_counts['count'].sum()
            df_counts['ä½”æ¯” (%)'] = (df_counts['count'] / total_rows_non_null * 100).round(2) if total_rows_non_null > 0 else 0
            # Handle potential non-string index values if column has mixed types wrongly classified as string
            try:
                df_counts.set_index(column_name, inplace=True)
            except Exception:
                 df_counts = df_counts.astype({column_name: str}) # Convert index column to string
                 df_counts.set_index(column_name, inplace=True)

            st.dataframe(df_counts)
            st.bar_chart(df_counts['count'])
        else:
             st.write("ç„¡æ³•å–å¾—å€¼åˆ†ä½ˆè³‡æ–™ã€‚")
    else: # distinct_count > 30 or distinct_count == 0 (already handled)
        st.write(f"#### éš¨æ©Ÿæ¨£æœ¬ (30 ç­†)")
        st.info(f"ç”±æ–¼ä¸é‡è¤‡å€¼æ•¸é‡ ({distinct_count}) éå¤šï¼Œåƒ…é¡¯ç¤ºéš¨æ©Ÿæ¨£æœ¬ã€‚")
        query_sample = f"SELECT DISTINCT \"{column_name}\" FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL LIMIT 30;"
        df_sample = run_query(db_type, query_sample)
        if not df_sample.empty:
            st.dataframe(df_sample)
        else:
             st.write("ç„¡æ³•å–å¾—æ¨£æœ¬è³‡æ–™ã€‚")

def analyze_boolean_column(db_type, schema_name, object_name, column_name, object_type):
    """Performs EDA for boolean columns."""
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
    if not column_name or not all(c in allowed_chars for c in column_name):
         st.error(f"ç„¡æ•ˆçš„æ¬„ä½åç¨±: {column_name}")
         return
    if not schema_name or not all(c in allowed_chars for c in schema_name):
        st.error(f"ç„¡æ•ˆçš„ Schema åç¨±: {schema_name}")
        return
    if not object_name or not all(c in allowed_chars for c in object_name):
         st.error(f"ç„¡æ•ˆçš„ç‰©ä»¶åç¨±: {object_name}")
         return

    qualified_name = f"\"{schema_name}\".\"{object_name}\""
    st.write("#### å€¼åˆ†ä½ˆ")
    query_counts = f"SELECT \"{column_name}\", COUNT(*) as count FROM {qualified_name} WHERE \"{column_name}\" IS NOT NULL GROUP BY \"{column_name}\";"
    df_counts = run_query(db_type, query_counts)

    if not df_counts.empty:
        total_rows_non_null = df_counts['count'].sum()
        df_counts['ä½”æ¯” (%)'] = (df_counts['count'] / total_rows_non_null * 100).round(2) if total_rows_non_null > 0 else 0
        df_counts[column_name] = df_counts[column_name].astype(str)
        df_counts.set_index(column_name, inplace=True)
        st.dataframe(df_counts)
        st.bar_chart(df_counts['count'])
    else:
        st.write("ç„¡æ³•å–å¾—å€¼åˆ†ä½ˆè³‡æ–™ã€‚")


# --- LLM Helper Functions ---
# (generate_translation_prompt remains the same)
def generate_translation_prompt(column_name):
    """Generates prompt for translating column name."""
    return f"è«‹å°‡ä»¥ä¸‹æŠ€è¡“æ€§çš„è³‡æ–™åº«æ¬„ä½åç¨±ç¿»è­¯æˆåœ¨å°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡æ¥­å‹™æ„ç¾©ï¼Œè«‹ç›¡é‡ç°¡çŸ­ï¼Œåªå›å‚³ç¿»è­¯çµæœï¼š\næ¬„ä½åç¨±ï¼š{column_name}\nå¯èƒ½çš„æ„ç¾©ï¼š"

# Modified to include object type
def generate_relations_prompt(schema_df, object_type):
    """Generates prompt for suggesting related columns."""
    schema_str = "\n".join([f"- {row['column_name']} ({row['data_type']})" for index, row in schema_df.iterrows()])
    return f"""
    ä»¥ä¸‹æ˜¯ä¸€å€‹è³‡æ–™åº« {object_type} çš„æ¬„ä½åŠå…¶è³‡æ–™é¡å‹ï¼š
    {schema_str}

    è«‹æ ¹æ“šé€™äº›æ¬„ä½åç¨±å’Œé¡å‹ï¼Œæ¨æ¸¬ä¸¦å»ºè­°å“ªäº›æ¬„ä½ä¹‹é–“å¯èƒ½å­˜åœ¨é—œè¯æ€§ï¼ˆä¾‹å¦‚ä¸»å¤–éµã€æ™‚é–“åºåˆ—ã€åˆ†é¡é—œä¿‚ï¼‰ï¼Œæˆ–è€…å“ªäº›æ¬„ä½çµ„åˆåœ¨ä¸€èµ·åˆ†æå¯èƒ½æœƒæœ‰æ„ç¾©ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡çŸ­è§£é‡‹åŸå› ã€‚

    å¯èƒ½çš„é—œè¯æˆ–çµ„åˆå»ºè­°ï¼š
    """

# --- Main Streamlit Function ---
def show(llm_client: LLMClientInterface, model_name: str):
    """ä¸»å‡½æ•¸ï¼Œé¡¯ç¤ºè³‡æ–™è¡¨æ¢ç´¢å·¥å…·çš„ä»‹é¢èˆ‡é‚è¼¯ã€‚"""
    st.markdown("##### è³‡æ–™è¡¨/è¦–åœ– æ¢ç´¢å·¥å…·")
    st.write("é¸æ“‡è³‡æ–™åº«ã€ç‰©ä»¶é¡å‹ã€Schema å’Œåç¨±ï¼Œæ¢ç´¢å…¶çµæ§‹ã€åŸºæœ¬è³‡è¨Šå’Œæ¬„ä½å…§å®¹åˆ†ä½ˆã€‚")

    # --- Inputs ---
    col1, col2 = st.columns(2)
    with col1:
        db_type = st.selectbox("é¸æ“‡è³‡æ–™åº«é¡å‹", options=list(DB_FUNCTIONS.keys()))
    with col2:
        object_type = st.radio("é¸æ“‡ç‰©ä»¶é¡å‹", ('TABLE', 'VIEW'), horizontal=True)

    # æ–°å¢ Schema è¼¸å…¥
    schema_name = st.text_input("è¼¸å…¥ Schema åç¨±", value="", help="ç‰©ä»¶æ‰€åœ¨çš„ Schema åç¨±ï¼Œä¾‹å¦‚ public")
    object_name = st.text_input(f"è¼¸å…¥è¦æ¢ç´¢çš„ {object_type} åç¨±", help=f"è«‹è¼¸å…¥ {schema_name} ä¸­çš„ {object_type} åç¨±ï¼Œæ³¨æ„å€åˆ†å¤§å°å¯«ã€‚")

    if st.button(f"ğŸš€ é–‹å§‹æ¢ç´¢ {object_type}", type="primary"):
        if not db_type: st.warning("è«‹é¸æ“‡è³‡æ–™åº«é¡å‹ã€‚"); st.stop()
        if not schema_name: st.warning("è«‹è¼¸å…¥ Schema åç¨±ã€‚"); st.stop() # æª¢æŸ¥ Schema åç¨±
        if not object_name: st.warning(f"è«‹è¼¸å…¥ {object_type} åç¨±ã€‚"); st.stop()

        with st.spinner(f"æ­£åœ¨é€£æ¥ {db_type} ä¸¦è®€å– {object_type} '{schema_name}.{object_name}' è³‡è¨Š..."):
            # --- 1. åŸºç¤è³‡è¨Š & Schema ---
            st.markdown("---")
            st.markdown(f"### 1. {object_type} åŸºç¤è³‡è¨Š & æ¬„ä½çµæ§‹")

            # å‚³é schema_name
            row_count = get_object_row_count(db_type, schema_name, object_name, object_type)
            st.metric("ç¸½è³‡æ–™ç­†æ•¸", f"{row_count:,}" if row_count is not None else "è¨ˆç®—å¤±æ•—/éä¹…")

            # å‚³é schema_name
            schema_df = get_object_schema(db_type, schema_name, object_name, object_type)

            if schema_df.empty:
                st.error(f"ç„¡æ³•è®€å– {object_type} '{schema_name}.{object_name}' çš„çµæ§‹ï¼Œè«‹ç¢ºèªåç¨±ã€æ¬Šé™å’Œç‰©ä»¶é¡å‹ã€‚")
                st.stop()

            st.write(f"ç¸½æ¬„ä½æ•¸: {len(schema_df)}")

            # --- LLM Translation ---
            translations = []
            meanings_placeholder = st.empty()
            with st.spinner("æ­£åœ¨é€é AI æ¨æ¸¬æ¬„ä½æ„ç¾©..."):
                if llm_client and model_name:
                    for index, row in schema_df.iterrows():
                        try:
                            prompt = generate_translation_prompt(row['column_name'])
                            translation = llm_client.generate_text(prompt, model_name, temperature=0.1)
                            translations.append(translation.strip())
                        except Exception as e:
                            print(f"ç¿»è­¯æ¬„ä½ {row['column_name']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            translations.append("ç¿»è­¯å¤±æ•—")
                    meanings_placeholder.success("æ¬„ä½æ„ç¾©æ¨æ¸¬å®Œæˆï¼")
                    schema_df['æ¨æ¸¬æ„ç¾© (AI)'] = translations
                else:
                    meanings_placeholder.warning("LLM æœå‹™æœªé…ç½®ï¼Œç„¡æ³•é€²è¡Œæ¬„ä½æ„ç¾©æ¨æ¸¬ã€‚")
                    schema_df['æ¨æ¸¬æ„ç¾© (AI)'] = "N/A"

            schema_df['é€šç”¨é¡å‹'] = schema_df['data_type'].apply(map_data_type)
            st.dataframe(schema_df)

            # --- LLM Relations Suggestion ---
            st.markdown("---")
            st.markdown("### 2. æ¬„ä½é—œè¯æ€§å»ºè­° (AI)")
            with st.spinner("æ­£åœ¨é€é AI åˆ†ææ¬„ä½é–“å¯èƒ½çš„é—œè¯..."):
                 if llm_client and model_name:
                     try:
                         relations_prompt = generate_relations_prompt(schema_df, object_type)
                         relations_suggestion = llm_client.generate_text(relations_prompt, model_name, temperature=0.5)
                         st.markdown(relations_suggestion)
                     except Exception as e:
                         st.error(f"åˆ†ææ¬„ä½é—œè¯æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                 else:
                     st.warning("LLM æœå‹™æœªé…ç½®ï¼Œç„¡æ³•é€²è¡Œæ¬„ä½é—œè¯å»ºè­°ã€‚")

            # --- EDA Section ---
            st.markdown("---")
            st.markdown("### 3. æ¬„ä½æ¢ç´¢æ€§åˆ†æ (EDA)")
            st.info("é»æ“Šå±•é–‹å„æ¬„ä½æŸ¥çœ‹è©³ç´°åˆ†æã€‚")

            if not schema_df.empty:
                for index, row in schema_df.iterrows():
                    col_name = row['column_name']
                    col_db_type = row['data_type']
                    col_general_type = row['é€šç”¨é¡å‹']

                    with st.expander(f"æ¬„ä½: **{col_name}** (é¡å‹: {col_db_type} / {col_general_type})"):
                        # å‚³é schema_name çµ¦ analyze å‡½æ•¸
                        with st.spinner(f"æ­£åœ¨åˆ†ææ¬„ä½ {col_name}..."):
                            try:
                                if col_general_type == 'datetime':
                                    analyze_datetime_column(db_type, schema_name, object_name, col_name, object_type)
                                elif col_general_type == 'numeric':
                                    analyze_numeric_column(db_type, schema_name, object_name, col_name, object_type)
                                elif col_general_type == 'string':
                                    analyze_string_column(db_type, schema_name, object_name, col_name, object_type)
                                elif col_general_type == 'boolean':
                                    analyze_boolean_column(db_type, schema_name, object_name, col_name, object_type)
                                else:
                                    st.write("æ­¤è³‡æ–™é¡å‹å°šä¸æ”¯æ´è‡ªå‹• EDA åˆ†æã€‚")
                            except Exception as e:
                                st.error(f"åˆ†ææ¬„ä½ '{col_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            else:
                st.warning("ç„¡æ³•åŸ·è¡Œ EDAï¼Œå› ç‚ºæœªèƒ½è®€å–è³‡æ–™è¡¨/è¦–åœ–çµæ§‹ã€‚")